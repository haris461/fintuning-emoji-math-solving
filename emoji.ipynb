{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54487ab4-41f9-4610-81d0-4fa37b14039f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37b9a04b-375a-424c-ab96-c1c5f9597a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f399434bb0410cabc4c1b7bb8c84fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mr.Laptop point\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 09:55, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.769600</td>\n",
       "      <td>2.948898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.090300</td>\n",
       "      <td>2.160397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.987400</td>\n",
       "      <td>1.613263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.356300</td>\n",
       "      <td>1.343794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.233900</td>\n",
       "      <td>1.230469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.168100</td>\n",
       "      <td>1.205306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.155900</td>\n",
       "      <td>1.187350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.908800</td>\n",
       "      <td>1.173637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.067600</td>\n",
       "      <td>1.171473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.000200</td>\n",
       "      <td>1.177170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.858800</td>\n",
       "      <td>1.183361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.869800</td>\n",
       "      <td>1.187717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.115600</td>\n",
       "      <td>1.189214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.933000</td>\n",
       "      <td>1.188654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.864400</td>\n",
       "      <td>1.189692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.085900</td>\n",
       "      <td>1.187015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.235800</td>\n",
       "      <td>1.187836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.780500</td>\n",
       "      <td>1.189064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.727100</td>\n",
       "      <td>1.189336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.990300</td>\n",
       "      <td>1.190310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the fine-tuned model...\n",
      "\n",
      "Testing the fine-tuned model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mr.Laptop point\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output: 🚗 + 🚗 + 🚗 + 🚗 = 20 -> 25 -> 30 -> 40 -> 50 -> 60 ->\n",
      "Warning: Model output '25' corrected to '🚗 = 5'\n",
      "Input: 🚗 + 🚗 + 🚗 + 🚗 = 20\n",
      "Output: 🚗 = 5\n",
      "\n",
      "Raw output: 🌵 + 🌵 + 🌵 = 15 -> 16 -> 18 -> 20 -> 30 -> 40 ->\n",
      "Warning: Model output '16' corrected to '🌵 = 5'\n",
      "Input: 🌵 + 🌵 + 🌵 = 15\n",
      "Output: 🌵 = 5\n",
      "\n",
      "Raw output: 🐱 + 🐱 = 10 -> 9 -> 8 -> 7 -> 6 -> 5 ->\n",
      "Warning: Model output '9' corrected to '🐱 = 5'\n",
      "Input: 🐱 + 🐱 = 10\n",
      "Output: 🐱 = 5\n",
      "\n",
      "Raw output: 🚗 + 🚗 = 16 -> 18 -> 20 -> 30 -> 40 -> 50 ->\n",
      "Warning: Model output '18' corrected to '🚗 = 8'\n",
      "Input: 🚗 + 🚗 = 16\n",
      "Output: 🚗 = 8\n",
      "\n",
      "Raw output: 🍔 + 🍔 = 14 -> 15 -> 16 -> 17 -> 18 -> 19 ->\n",
      "Warning: Model output '15' corrected to '🍔 = 7'\n",
      "Input: 🍔 + 🍔 = 14\n",
      "Output: 🍔 = 7\n",
      "\n",
      "Raw output: 🎤 + 🎤 = 8 -> 9 -> 10 -> 11 -> 12 -> 13 ->\n",
      "Warning: Model output '9' corrected to '🎤 = 4'\n",
      "Input: 🎤 + 🎤 = 8\n",
      "Output: 🎤 = 4\n",
      "\n",
      "Raw output: 🏡 + 🏡 + 🏡 = 21 -> 22 -> 23 -> 24 -> 25 -> 26 ->\n",
      "Warning: Model output '22' corrected to '🏡 = 7'\n",
      "Input: 🏡 + 🏡 + 🏡 = 21\n",
      "Output: 🏡 = 7\n",
      "\n",
      "Raw output: 🦁 + 🦁 = 18 -> 19 -> 20 -> 21 -> 22 -> 23 ->\n",
      "Warning: Model output '19' corrected to '🦁 = 9'\n",
      "Input: 🦁 + 🦁 = 18\n",
      "Output: 🦁 = 9\n",
      "\n",
      "Raw output: 🦒 + 🦒 = 10 -> 11 -> 12 -> 13 -> 14 -> 15 ->\n",
      "Warning: Model output '11' corrected to '🦒 = 5'\n",
      "Input: 🦒 + 🦒 = 10\n",
      "Output: 🦒 = 5\n",
      "\n",
      "Raw output: 🌈 + 🌈 + 🌈 = 18 -> 20 -> 30 -> 40 -> 50 -> 60 ->\n",
      "Warning: Model output '20' corrected to '🌈 = 6'\n",
      "Input: 🌈 + 🌈 + 🌈 = 18\n",
      "Output: 🌈 = 6\n",
      "\n",
      "Raw output: 🔥 + 🔥 + 🔥 = 27 -> 28 -> 29 -> 30 -> 31 -> 32 ->\n",
      "Warning: Model output '28' corrected to '🔥 = 9'\n",
      "Input: 🔥 + 🔥 + 🔥 = 27\n",
      "Output: 🔥 = 9\n",
      "\n",
      "Raw output: 🐼 + 🐼 = 20 -> 25 -> 30 -> 40 -> 50 -> 60 ->\n",
      "Warning: Model output '25' corrected to '🐼 = 10'\n",
      "Input: 🐼 + 🐼 = 20\n",
      "Output: 🐼 = 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "# Load dataset from CSV\n",
    "df = pd.read_csv(\"emoji_math_dataset.csv\")\n",
    "dataset = [{\"problem\": problem, \"solution\": solution} for problem, solution in zip(df[\"Problem\"], df[\"Solution\"])]\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_list(dataset)\n",
    "\n",
    "# Choose model\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization function: Focus on solution as label\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer([f\"{p} ->\" for p in examples[\"problem\"]], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    labels = tokenizer(examples[\"solution\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"labels\": labels[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"problem\", \"solution\"])\n",
    "\n",
    "# Split dataset into train and eval (80% train, 20% eval)\n",
    "train_size = int(0.8 * len(tokenized_datasets))\n",
    "train_dataset = tokenized_datasets.select(range(train_size))\n",
    "eval_dataset = tokenized_datasets.select(range(train_size, len(tokenized_datasets)))\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./emoji-math-model\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=20,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save fine-tuned model\n",
    "model.save_pretrained(\"./emoji-math-model\")\n",
    "tokenizer.save_pretrained(\"./emoji-math-model\")\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "print(\"\\nLoading the fine-tuned model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./emoji-math-model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./emoji-math-model\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Inference function with validation\n",
    "def solve_emoji_math(equation):\n",
    "    model.eval()\n",
    "    input_text = f\"{equation} ->\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_beams=10,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=False,\n",
    "            temperature=0.1\n",
    "        )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    solution = result.split(\"->\")[1].strip()\n",
    "    print(f\"Raw output: {result}\")\n",
    "    \n",
    "    # Post-processing to ensure mathematical accuracy\n",
    "    emoji = equation.split()[0]\n",
    "    count = equation.count(emoji)\n",
    "    total = int(equation.split(\"=\")[1].strip())\n",
    "    expected_value = total // count\n",
    "    if f\"{emoji} = {expected_value}\" != solution:\n",
    "        print(f\"Warning: Model output '{solution}' corrected to '{emoji} = {expected_value}'\")\n",
    "        solution = f\"{emoji} = {expected_value}\"\n",
    "    return solution\n",
    "\n",
    "# Test the model\n",
    "test_equations = [\n",
    "    \"🚗 + 🚗 + 🚗 + 🚗 = 20\",  # Should be 🚗 = 5\n",
    "    \"🌵 + 🌵 + 🌵 = 15\",      # Should be 🌵 = 5\n",
    "    \"🐱 + 🐱 = 10\",           # Should be 🐱 = 5\n",
    "    \"🚗 + 🚗 = 16\",           # Should be 🚗 = 8\n",
    "    \"🍔 + 🍔 = 14\",           # Should be 🍔 = 7\n",
    "    \"🎤 + 🎤 = 8\",            # Should be 🎤 = 4\n",
    "    \"🏡 + 🏡 + 🏡 = 21\",      # Should be 🏡 = 7\n",
    "    \"🦁 + 🦁 = 18\",           # Should be 🦁 = 9\n",
    "    \"🦒 + 🦒 = 10\",           # Should be 🦒 = 5\n",
    "    \"🌈 + 🌈 + 🌈 = 18\",      # Should be 🌈 = 6\n",
    "    \"🔥 + 🔥 + 🔥 = 27\",      # Should be 🔥 = 9\n",
    "    \"🐼 + 🐼 = 20\",           # Should be 🐼 = 10\n",
    "]\n",
    "\n",
    "print(\"\\nTesting the fine-tuned model:\")\n",
    "for eq in test_equations:\n",
    "    solution = solve_emoji_math(eq)\n",
    "    print(f\"Input: {eq}\")\n",
    "    print(f\"Output: {solution}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d340e3fb-6458-4c9d-b996-d80d35f54dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the fine-tuned model...\n",
      "\n",
      "Testing the fine-tuned model:\n",
      "Raw output: 🚗 + 🚗 + 🚗 + 🚗 = 20 -> 25 -> 30 -> 40 -> 50 -> 60 ->\n",
      "Warning: Model output '25' corrected to '🚗 = 5'\n",
      "Input: 🚗 + 🚗 + 🚗 + 🚗 = 20\n",
      "Output: 🚗 = 5\n",
      "\n",
      "Raw output: 🌵 + 🌵 + 🌵 = 15 -> 16 -> 18 -> 20 -> 30 -> 40 ->\n",
      "Warning: Model output '16' corrected to '🌵 = 5'\n",
      "Input: 🌵 + 🌵 + 🌵 = 15\n",
      "Output: 🌵 = 5\n",
      "\n",
      "Raw output: 🐱 + 🐱 = 10 -> 9 -> 8 -> 7 -> 6 -> 5 ->\n",
      "Warning: Model output '9' corrected to '🐱 = 5'\n",
      "Input: 🐱 + 🐱 = 10\n",
      "Output: 🐱 = 5\n",
      "\n",
      "Raw output: 🚗 + 🚗 = 16 -> 18 -> 20 -> 30 -> 40 -> 50 ->\n",
      "Warning: Model output '18' corrected to '🚗 = 8'\n",
      "Input: 🚗 + 🚗 = 16\n",
      "Output: 🚗 = 8\n",
      "\n",
      "Raw output: 🍔 + 🍔 = 14 -> 15 -> 16 -> 17 -> 18 -> 19 ->\n",
      "Warning: Model output '15' corrected to '🍔 = 7'\n",
      "Input: 🍔 + 🍔 = 14\n",
      "Output: 🍔 = 7\n",
      "\n",
      "Raw output: 🎤 + 🎤 = 8 -> 9 -> 10 -> 11 -> 12 -> 13 ->\n",
      "Warning: Model output '9' corrected to '🎤 = 4'\n",
      "Input: 🎤 + 🎤 = 8\n",
      "Output: 🎤 = 4\n",
      "\n",
      "Raw output: 🏡 + 🏡 + 🏡 = 21 -> 22 -> 23 -> 24 -> 25 -> 26 ->\n",
      "Warning: Model output '22' corrected to '🏡 = 7'\n",
      "Input: 🏡 + 🏡 + 🏡 = 21\n",
      "Output: 🏡 = 7\n",
      "\n",
      "Raw output: 🦁 + 🦁 = 18 -> 19 -> 20 -> 21 -> 22 -> 23 ->\n",
      "Warning: Model output '19' corrected to '🦁 = 9'\n",
      "Input: 🦁 + 🦁 = 18\n",
      "Output: 🦁 = 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load fine-tuned model\n",
    "print(\"\\nLoading the fine-tuned model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./emoji-math-model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./emoji-math-model\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Inference function with validation\n",
    "def solve_emoji_math(equation):\n",
    "    model.eval()\n",
    "    input_text = f\"{equation} ->\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_beams=10,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=False,\n",
    "            temperature=0.1\n",
    "        )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    solution = result.split(\"->\")[1].strip()\n",
    "    print(f\"Raw output: {result}\")\n",
    "    \n",
    "    # Post-processing to ensure mathematical accuracy\n",
    "    emoji = equation.split()[0]\n",
    "    count = equation.count(emoji)\n",
    "    total = int(equation.split(\"=\")[1].strip())\n",
    "    expected_value = total // count\n",
    "    if f\"{emoji} = {expected_value}\" != solution:\n",
    "        print(f\"Warning: Model output '{solution}' corrected to '{emoji} = {expected_value}'\")\n",
    "        solution = f\"{emoji} = {expected_value}\"\n",
    "    return solution\n",
    "\n",
    "# Test the model\n",
    "test_equations = [\n",
    "    \"🚗 + 🚗 + 🚗 + 🚗 = 20\",  # Should be 🚗 = 5\n",
    "    \"🌵 + 🌵 + 🌵 = 15\",      # Should be 🌵 = 5\n",
    "    \"🐱 + 🐱 = 10\",           # Should be 🐱 = 5\n",
    "    \"🚗 + 🚗 = 16\",           # Should be 🚗 = 8\n",
    "    \"🍔 + 🍔 = 14\",           # Should be 🍔 = 7\n",
    "    \"🎤 + 🎤 = 8\",            # Should be 🎤 = 4\n",
    "    \"🏡 + 🏡 + 🏡 = 21\",      # Should be 🏡 = 7\n",
    "    \"🦁 + 🦁 = 18\",           # Should be 🦁 = 9\n",
    "\n",
    "]\n",
    "\n",
    "print(\"\\nTesting the fine-tuned model:\")\n",
    "for eq in test_equations:\n",
    "    solution = solve_emoji_math(eq)\n",
    "    print(f\"Input: {eq}\")\n",
    "    print(f\"Output: {solution}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd10e717-c9ef-479c-9246-e842f285d5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20746079-5099-41af-9130-729a278b488e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the fine-tuned model...\n",
      "\n",
      "Testing the fine-tuned model:\n",
      "Raw output: 🚗 + 🚗 + 🚗 + 🚗 = 20 -> 👍 -> � = 5 -> 😍\n",
      "Input: 🚗 + 🚗 + 🚗 + 🚗 = 20\n",
      "Output: 👍\n",
      "\n",
      "Raw output: 🌵 + 🌵 + 🌵 = 15 -> 🎉 = 5 -> 👎 = 3\n",
      "Input: 🌵 + 🌵 + 🌵 = 15\n",
      "Output: 🎉 = 5\n",
      "\n",
      "Raw output: 🐱 + 🐱 = 10 -> 😀 = 5 -> 💱 += �\n",
      "Input: 🐱 + 🐱 = 10\n",
      "Output: 😀 = 5\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ed1f0-507c-4647-a0ab-dc2eaed3cc89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
