{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54487ab4-41f9-4610-81d0-4fa37b14039f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37b9a04b-375a-424c-ab96-c1c5f9597a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f399434bb0410cabc4c1b7bb8c84fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mr.Laptop point\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 09:55, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.769600</td>\n",
       "      <td>2.948898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.090300</td>\n",
       "      <td>2.160397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.987400</td>\n",
       "      <td>1.613263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.356300</td>\n",
       "      <td>1.343794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.233900</td>\n",
       "      <td>1.230469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.168100</td>\n",
       "      <td>1.205306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.155900</td>\n",
       "      <td>1.187350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.908800</td>\n",
       "      <td>1.173637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.067600</td>\n",
       "      <td>1.171473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.000200</td>\n",
       "      <td>1.177170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.858800</td>\n",
       "      <td>1.183361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.869800</td>\n",
       "      <td>1.187717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.115600</td>\n",
       "      <td>1.189214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.933000</td>\n",
       "      <td>1.188654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.864400</td>\n",
       "      <td>1.189692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.085900</td>\n",
       "      <td>1.187015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.235800</td>\n",
       "      <td>1.187836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.780500</td>\n",
       "      <td>1.189064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.727100</td>\n",
       "      <td>1.189336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.990300</td>\n",
       "      <td>1.190310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the fine-tuned model...\n",
      "\n",
      "Testing the fine-tuned model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mr.Laptop point\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output: üöó + üöó + üöó + üöó = 20 -> 25 -> 30 -> 40 -> 50 -> 60 ->\n",
      "Warning: Model output '25' corrected to 'üöó = 5'\n",
      "Input: üöó + üöó + üöó + üöó = 20\n",
      "Output: üöó = 5\n",
      "\n",
      "Raw output: üåµ + üåµ + üåµ = 15 -> 16 -> 18 -> 20 -> 30 -> 40 ->\n",
      "Warning: Model output '16' corrected to 'üåµ = 5'\n",
      "Input: üåµ + üåµ + üåµ = 15\n",
      "Output: üåµ = 5\n",
      "\n",
      "Raw output: üê± + üê± = 10 -> 9 -> 8 -> 7 -> 6 -> 5 ->\n",
      "Warning: Model output '9' corrected to 'üê± = 5'\n",
      "Input: üê± + üê± = 10\n",
      "Output: üê± = 5\n",
      "\n",
      "Raw output: üöó + üöó = 16 -> 18 -> 20 -> 30 -> 40 -> 50 ->\n",
      "Warning: Model output '18' corrected to 'üöó = 8'\n",
      "Input: üöó + üöó = 16\n",
      "Output: üöó = 8\n",
      "\n",
      "Raw output: üçî + üçî = 14 -> 15 -> 16 -> 17 -> 18 -> 19 ->\n",
      "Warning: Model output '15' corrected to 'üçî = 7'\n",
      "Input: üçî + üçî = 14\n",
      "Output: üçî = 7\n",
      "\n",
      "Raw output: üé§ + üé§ = 8 -> 9 -> 10 -> 11 -> 12 -> 13 ->\n",
      "Warning: Model output '9' corrected to 'üé§ = 4'\n",
      "Input: üé§ + üé§ = 8\n",
      "Output: üé§ = 4\n",
      "\n",
      "Raw output: üè° + üè° + üè° = 21 -> 22 -> 23 -> 24 -> 25 -> 26 ->\n",
      "Warning: Model output '22' corrected to 'üè° = 7'\n",
      "Input: üè° + üè° + üè° = 21\n",
      "Output: üè° = 7\n",
      "\n",
      "Raw output: ü¶Å + ü¶Å = 18 -> 19 -> 20 -> 21 -> 22 -> 23 ->\n",
      "Warning: Model output '19' corrected to 'ü¶Å = 9'\n",
      "Input: ü¶Å + ü¶Å = 18\n",
      "Output: ü¶Å = 9\n",
      "\n",
      "Raw output: ü¶í + ü¶í = 10 -> 11 -> 12 -> 13 -> 14 -> 15 ->\n",
      "Warning: Model output '11' corrected to 'ü¶í = 5'\n",
      "Input: ü¶í + ü¶í = 10\n",
      "Output: ü¶í = 5\n",
      "\n",
      "Raw output: üåà + üåà + üåà = 18 -> 20 -> 30 -> 40 -> 50 -> 60 ->\n",
      "Warning: Model output '20' corrected to 'üåà = 6'\n",
      "Input: üåà + üåà + üåà = 18\n",
      "Output: üåà = 6\n",
      "\n",
      "Raw output: üî• + üî• + üî• = 27 -> 28 -> 29 -> 30 -> 31 -> 32 ->\n",
      "Warning: Model output '28' corrected to 'üî• = 9'\n",
      "Input: üî• + üî• + üî• = 27\n",
      "Output: üî• = 9\n",
      "\n",
      "Raw output: üêº + üêº = 20 -> 25 -> 30 -> 40 -> 50 -> 60 ->\n",
      "Warning: Model output '25' corrected to 'üêº = 10'\n",
      "Input: üêº + üêº = 20\n",
      "Output: üêº = 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "# Load dataset from CSV\n",
    "df = pd.read_csv(\"emoji_math_dataset.csv\")\n",
    "dataset = [{\"problem\": problem, \"solution\": solution} for problem, solution in zip(df[\"Problem\"], df[\"Solution\"])]\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_list(dataset)\n",
    "\n",
    "# Choose model\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization function: Focus on solution as label\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer([f\"{p} ->\" for p in examples[\"problem\"]], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    labels = tokenizer(examples[\"solution\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"labels\": labels[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"problem\", \"solution\"])\n",
    "\n",
    "# Split dataset into train and eval (80% train, 20% eval)\n",
    "train_size = int(0.8 * len(tokenized_datasets))\n",
    "train_dataset = tokenized_datasets.select(range(train_size))\n",
    "eval_dataset = tokenized_datasets.select(range(train_size, len(tokenized_datasets)))\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./emoji-math-model\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=20,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save fine-tuned model\n",
    "model.save_pretrained(\"./emoji-math-model\")\n",
    "tokenizer.save_pretrained(\"./emoji-math-model\")\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "print(\"\\nLoading the fine-tuned model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./emoji-math-model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./emoji-math-model\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Inference function with validation\n",
    "def solve_emoji_math(equation):\n",
    "    model.eval()\n",
    "    input_text = f\"{equation} ->\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_beams=10,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=False,\n",
    "            temperature=0.1\n",
    "        )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    solution = result.split(\"->\")[1].strip()\n",
    "    print(f\"Raw output: {result}\")\n",
    "    \n",
    "    # Post-processing to ensure mathematical accuracy\n",
    "    emoji = equation.split()[0]\n",
    "    count = equation.count(emoji)\n",
    "    total = int(equation.split(\"=\")[1].strip())\n",
    "    expected_value = total // count\n",
    "    if f\"{emoji} = {expected_value}\" != solution:\n",
    "        print(f\"Warning: Model output '{solution}' corrected to '{emoji} = {expected_value}'\")\n",
    "        solution = f\"{emoji} = {expected_value}\"\n",
    "    return solution\n",
    "\n",
    "# Test the model\n",
    "test_equations = [\n",
    "    \"üöó + üöó + üöó + üöó = 20\",  # Should be üöó = 5\n",
    "    \"üåµ + üåµ + üåµ = 15\",      # Should be üåµ = 5\n",
    "    \"üê± + üê± = 10\",           # Should be üê± = 5\n",
    "    \"üöó + üöó = 16\",           # Should be üöó = 8\n",
    "    \"üçî + üçî = 14\",           # Should be üçî = 7\n",
    "    \"üé§ + üé§ = 8\",            # Should be üé§ = 4\n",
    "    \"üè° + üè° + üè° = 21\",      # Should be üè° = 7\n",
    "    \"ü¶Å + ü¶Å = 18\",           # Should be ü¶Å = 9\n",
    "    \"ü¶í + ü¶í = 10\",           # Should be ü¶í = 5\n",
    "    \"üåà + üåà + üåà = 18\",      # Should be üåà = 6\n",
    "    \"üî• + üî• + üî• = 27\",      # Should be üî• = 9\n",
    "    \"üêº + üêº = 20\",           # Should be üêº = 10\n",
    "]\n",
    "\n",
    "print(\"\\nTesting the fine-tuned model:\")\n",
    "for eq in test_equations:\n",
    "    solution = solve_emoji_math(eq)\n",
    "    print(f\"Input: {eq}\")\n",
    "    print(f\"Output: {solution}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d340e3fb-6458-4c9d-b996-d80d35f54dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the fine-tuned model...\n",
      "\n",
      "Testing the fine-tuned model:\n",
      "Raw output: üöó + üöó + üöó + üöó = 20 -> 25 -> 30 -> 40 -> 50 -> 60 ->\n",
      "Warning: Model output '25' corrected to 'üöó = 5'\n",
      "Input: üöó + üöó + üöó + üöó = 20\n",
      "Output: üöó = 5\n",
      "\n",
      "Raw output: üåµ + üåµ + üåµ = 15 -> 16 -> 18 -> 20 -> 30 -> 40 ->\n",
      "Warning: Model output '16' corrected to 'üåµ = 5'\n",
      "Input: üåµ + üåµ + üåµ = 15\n",
      "Output: üåµ = 5\n",
      "\n",
      "Raw output: üê± + üê± = 10 -> 9 -> 8 -> 7 -> 6 -> 5 ->\n",
      "Warning: Model output '9' corrected to 'üê± = 5'\n",
      "Input: üê± + üê± = 10\n",
      "Output: üê± = 5\n",
      "\n",
      "Raw output: üöó + üöó = 16 -> 18 -> 20 -> 30 -> 40 -> 50 ->\n",
      "Warning: Model output '18' corrected to 'üöó = 8'\n",
      "Input: üöó + üöó = 16\n",
      "Output: üöó = 8\n",
      "\n",
      "Raw output: üçî + üçî = 14 -> 15 -> 16 -> 17 -> 18 -> 19 ->\n",
      "Warning: Model output '15' corrected to 'üçî = 7'\n",
      "Input: üçî + üçî = 14\n",
      "Output: üçî = 7\n",
      "\n",
      "Raw output: üé§ + üé§ = 8 -> 9 -> 10 -> 11 -> 12 -> 13 ->\n",
      "Warning: Model output '9' corrected to 'üé§ = 4'\n",
      "Input: üé§ + üé§ = 8\n",
      "Output: üé§ = 4\n",
      "\n",
      "Raw output: üè° + üè° + üè° = 21 -> 22 -> 23 -> 24 -> 25 -> 26 ->\n",
      "Warning: Model output '22' corrected to 'üè° = 7'\n",
      "Input: üè° + üè° + üè° = 21\n",
      "Output: üè° = 7\n",
      "\n",
      "Raw output: ü¶Å + ü¶Å = 18 -> 19 -> 20 -> 21 -> 22 -> 23 ->\n",
      "Warning: Model output '19' corrected to 'ü¶Å = 9'\n",
      "Input: ü¶Å + ü¶Å = 18\n",
      "Output: ü¶Å = 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load fine-tuned model\n",
    "print(\"\\nLoading the fine-tuned model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./emoji-math-model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./emoji-math-model\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Inference function with validation\n",
    "def solve_emoji_math(equation):\n",
    "    model.eval()\n",
    "    input_text = f\"{equation} ->\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_beams=10,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=False,\n",
    "            temperature=0.1\n",
    "        )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    solution = result.split(\"->\")[1].strip()\n",
    "    print(f\"Raw output: {result}\")\n",
    "    \n",
    "    # Post-processing to ensure mathematical accuracy\n",
    "    emoji = equation.split()[0]\n",
    "    count = equation.count(emoji)\n",
    "    total = int(equation.split(\"=\")[1].strip())\n",
    "    expected_value = total // count\n",
    "    if f\"{emoji} = {expected_value}\" != solution:\n",
    "        print(f\"Warning: Model output '{solution}' corrected to '{emoji} = {expected_value}'\")\n",
    "        solution = f\"{emoji} = {expected_value}\"\n",
    "    return solution\n",
    "\n",
    "# Test the model\n",
    "test_equations = [\n",
    "    \"üöó + üöó + üöó + üöó = 20\",  # Should be üöó = 5\n",
    "    \"üåµ + üåµ + üåµ = 15\",      # Should be üåµ = 5\n",
    "    \"üê± + üê± = 10\",           # Should be üê± = 5\n",
    "    \"üöó + üöó = 16\",           # Should be üöó = 8\n",
    "    \"üçî + üçî = 14\",           # Should be üçî = 7\n",
    "    \"üé§ + üé§ = 8\",            # Should be üé§ = 4\n",
    "    \"üè° + üè° + üè° = 21\",      # Should be üè° = 7\n",
    "    \"ü¶Å + ü¶Å = 18\",           # Should be ü¶Å = 9\n",
    "\n",
    "]\n",
    "\n",
    "print(\"\\nTesting the fine-tuned model:\")\n",
    "for eq in test_equations:\n",
    "    solution = solve_emoji_math(eq)\n",
    "    print(f\"Input: {eq}\")\n",
    "    print(f\"Output: {solution}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd10e717-c9ef-479c-9246-e842f285d5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20746079-5099-41af-9130-729a278b488e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the fine-tuned model...\n",
      "\n",
      "Testing the fine-tuned model:\n",
      "Raw output: üöó + üöó + üöó + üöó = 20 -> üëç -> ÔøΩ = 5 -> üòç\n",
      "Input: üöó + üöó + üöó + üöó = 20\n",
      "Output: üëç\n",
      "\n",
      "Raw output: üåµ + üåµ + üåµ = 15 -> üéâ = 5 -> üëé = 3\n",
      "Input: üåµ + üåµ + üåµ = 15\n",
      "Output: üéâ = 5\n",
      "\n",
      "Raw output: üê± + üê± = 10 -> üòÄ = 5 -> üí± += ÔøΩ\n",
      "Input: üê± + üê± = 10\n",
      "Output: üòÄ = 5\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ed1f0-507c-4647-a0ab-dc2eaed3cc89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
